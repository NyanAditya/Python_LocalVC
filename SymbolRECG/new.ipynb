{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Start of Combined Code ---\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "import pdfplumber\n",
    "# import pytesseract # REMOVED\n",
    "import easyocr      # ADDED\n",
    "from PIL import Image\n",
    "import numpy as np  # ADDED (needed for image conversion for EasyOCR)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from ipywidgets import FileUpload, Button, IntSlider, RadioButtons, Output, VBox, HBox, Label, Layout\n",
    "from IPython.display import display, clear_output\n",
    "import traceback\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Summarization Model\n",
    "print(\"Loading DistilBART model for faster summarization...\")\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "print(f\"Model loaded successfully on {device}!\")\n",
    "\n",
    "# --- Initialize EasyOCR Reader ---\n",
    "print(\"Initializing EasyOCR Reader (this may take a moment on first run)...\")\n",
    "# Use gpu=True if torch/CUDA is available and configured, otherwise False\n",
    "# Add other languages if needed, e.g., ['en', 'fr']\n",
    "try:\n",
    "    ocr_reader = easyocr.Reader(['en'], gpu=(device == \"cuda\"))\n",
    "    print(\"‚úÖ EasyOCR Reader initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error initializing EasyOCR: {e}\")\n",
    "    print(\"   Will attempt to proceed without OCR on images.\")\n",
    "    ocr_reader = None # Set reader to None if initialization fails\n",
    "# --- End EasyOCR Initialization ---\n",
    "\n",
    "\n",
    "# Global variable to store the PDF path\n",
    "pdf_path = None\n",
    "\n",
    "# Set OCR resolution (still relevant for image quality fed to EasyOCR)\n",
    "OCR_RESOLUTION = 300\n",
    "\n",
    "# --- REMOVED TESSERACT CONFIGURATION BLOCK ---\n",
    "\n",
    "# Map of common math symbols (kept for math detection)\n",
    "MATH_SYMBOLS = {\n",
    "    '‚à´': '\\\\int', '‚àë': '\\\\sum', '‚àè': '\\\\prod', '‚àö': '\\\\sqrt', '‚àû': '\\\\infty', '‚â†': '\\\\neq',\n",
    "    '‚â§': '\\\\leq', '‚â•': '\\\\geq', '¬±': '\\\\pm', '‚Üí': '\\\\to', '‚àÇ': '\\\\partial', '‚àá': '\\\\nabla',\n",
    "    'œÄ': '\\\\pi', 'Œ∏': '\\\\theta', 'Œª': '\\\\lambda', 'Œº': '\\\\mu', 'œÉ': '\\\\sigma', 'œâ': '\\\\omega',\n",
    "    'Œ±': '\\\\alpha', 'Œ≤': '\\\\beta', 'Œ≥': '\\\\gamma', 'Œ¥': '\\\\delta', 'Œµ': '\\\\epsilon', '‚àà': '\\\\in',\n",
    "    '‚äÇ': '\\\\subset', '‚äÜ': '\\\\subseteq', '‚à™': '\\\\cup', '‚à©': '\\\\cap', '‚ü®': '\\\\langle', '‚ü©': '\\\\rangle',\n",
    "}\n",
    "\n",
    "# --- MODIFIED extract_text_from_pdf ---\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file, including OCR for images using EasyOCR.\"\"\"\n",
    "    global ocr_reader # Access the global reader\n",
    "    print(f\"Extracting text from {pdf_path}...\")\n",
    "    extracted_text = \"\"\n",
    "    if ocr_reader is None:\n",
    "        print(\"‚ö†Ô∏è EasyOCR Reader not available. Skipping OCR on images.\")\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            total_pages = len(pdf.pages)\n",
    "            print(f\"PDF has {total_pages} pages.\")\n",
    "            for page_num, page in enumerate(pdf.pages):\n",
    "                print(f\"Processing page {page_num+1}/{total_pages}...\")\n",
    "\n",
    "                # Extract regular text\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    extracted_text += page_text + \"\\n\"\n",
    "\n",
    "                # Process images on the page using EasyOCR if available\n",
    "                if page.images and ocr_reader: # Only process if reader is initialized\n",
    "                    print(f\"  Found {len(page.images)} image areas on page {page_num+1}. Processing OCR with EasyOCR...\")\n",
    "                    for img_num, img in enumerate(page.images):\n",
    "                        try:\n",
    "                            # Clamp the bounding box\n",
    "                            x0, top, x1, bottom = max(0, img['x0']), max(0, img['top']), min(page.width, img['x1']), min(page.height, img['bottom'])\n",
    "                            if x1 <= x0 or bottom <= top: continue # Skip invalid bbox\n",
    "\n",
    "                            bbox = (x0, top, x1, bottom)\n",
    "                            cropped = page.crop(bbox)\n",
    "                            pil_image = cropped.to_image(resolution=OCR_RESOLUTION).original\n",
    "\n",
    "                            # Convert PIL image to NumPy array needed by EasyOCR\n",
    "                            img_np = np.array(pil_image)\n",
    "\n",
    "                            # Perform OCR using EasyOCR\n",
    "                            # detail=0 returns just text, paragraph=True tries to combine lines\n",
    "                            ocr_results = ocr_reader.readtext(img_np, detail=0, paragraph=True)\n",
    "\n",
    "                            # Join the detected text paragraphs/lines\n",
    "                            ocr_text = \"\\n\".join(ocr_results)\n",
    "\n",
    "                            if ocr_text and ocr_text.strip():\n",
    "                                # print(f\"    EasyOCR Result (img {img_num+1}): '{ocr_text[:50].strip()}...'\") # Debug\n",
    "                                extracted_text += \" \" + ocr_text.strip() + \"\\n\"\n",
    "\n",
    "                        # Specific exceptions for EasyOCR or image processing can be added here if needed\n",
    "                        except Exception as e:\n",
    "                            print(f\"  Error processing image {img_num+1} with EasyOCR on page {page_num+1}: {type(e).__name__} - {str(e)}\")\n",
    "                            continue # Skip to next image\n",
    "                elif page.images and not ocr_reader:\n",
    "                     print(f\"  Skipping {len(page.images)} image areas on page {page_num+1} (EasyOCR unavailable).\")\n",
    "\n",
    "\n",
    "            print(f\"Finished processing pages. Extracted approx. {len(extracted_text)} characters.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening or processing PDF '{pdf_path}': {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return extracted_text\n",
    "# --- END MODIFIED extract_text_from_pdf ---\n",
    "\n",
    "\n",
    "# --- detect_math_content, clean_text, split_text_into_chunks functions remain the same ---\n",
    "def detect_math_content(text):\n",
    "    \"\"\"Detect if the document contains mathematical content using refined indicators.\"\"\"\n",
    "    if not text: return False\n",
    "    strong_indicators = [\n",
    "        r'[\\+\\-\\*\\/=<>‚â§‚â•‚â†¬±]', r'‚à´|‚àë|‚àè|‚àö|‚àû|‚àÇ|‚àá|‚àà|‚äÇ|‚äÜ|‚à™|‚à©',\n",
    "        r'\\b(equation|formula|theorem|lemma|proof|calculus|algebra|matrix|vector|integral|derivative)\\b',\n",
    "        r'\\^\\s*\\{?[0-9\\.\\-a-zA-Z]+\\}?', r'_\\s*\\{?[0-9a-zA-Z]+\\}?', r'\\\\frac|\\\\sqrt|\\\\sum|\\\\int|\\\\lim',\n",
    "        r'[a-zA-Z]\\s*\\(.*\\)\\s*=', r'\\b(sin|cos|tan|log|exp)\\b\\(',\n",
    "    ]\n",
    "    for pattern in strong_indicators:\n",
    "        if re.search(pattern, text, re.IGNORECASE): return True\n",
    "    for symbol in MATH_SYMBOLS.keys():\n",
    "        if symbol in text: return True\n",
    "    if re.search(r'\\b[a-zA-Z]+\\([a-zA-Z0-9,\\s]+\\)', text): return True\n",
    "    return False\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted text by removing unwanted spaces, characters, and fixing common OCR/extraction issues.\"\"\"\n",
    "    if not text: return \"\"\n",
    "    text = text.replace('Ô¨Å', 'fi').replace('Ô¨Ç', 'fl')\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text)\n",
    "    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
    "    text = re.sub(r'([.,!?;:])(?=\\w)', r'\\1 ', text)\n",
    "    text = re.sub(r'(\\w)-\\n(\\w)', r'\\1\\2', text)\n",
    "    text = re.sub(r'[\\f\\v]', '', text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=700, overlap=100): # Kept smaller chunk size\n",
    "    \"\"\"Split text into potentially overlapping chunks of a specified token size, respecting sentences.\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    total_tokens = len(tokens)\n",
    "    if total_tokens == 0: return []\n",
    "    chunks = []\n",
    "    start_token = 0\n",
    "    while start_token < total_tokens:\n",
    "        end_token = min(start_token + chunk_size, total_tokens)\n",
    "        current_chunk_tokens = tokens[start_token:end_token]\n",
    "        current_chunk_text = tokenizer.decode(current_chunk_tokens)\n",
    "        if end_token < total_tokens and (total_tokens - start_token) > chunk_size:\n",
    "             search_area_start = min(start_token + chunk_size + overlap, total_tokens)\n",
    "             search_text = tokenizer.decode(tokens[start_token:search_area_start])\n",
    "             sentence_end_indices = [m.start() for m in re.finditer(r'[.!?]\\s+', search_text)]\n",
    "             if sentence_end_indices:\n",
    "                  best_end_pos = -1\n",
    "                  for idx in reversed(sentence_end_indices):\n",
    "                      estimated_tokens = len(tokenizer.encode(search_text[:idx+1], add_special_tokens=False))\n",
    "                      if estimated_tokens <= chunk_size:\n",
    "                          best_end_pos = idx + 1\n",
    "                          break\n",
    "                  if best_end_pos != -1:\n",
    "                      adjusted_chunk_text = search_text[:best_end_pos]\n",
    "                      adjusted_tokens = tokenizer.encode(adjusted_chunk_text, add_special_tokens=False)\n",
    "                      if len(adjusted_tokens) > 0:\n",
    "                         current_chunk_text = adjusted_chunk_text\n",
    "                         end_token = start_token + len(adjusted_tokens)\n",
    "        chunks.append(current_chunk_text)\n",
    "        next_start_token = end_token - overlap\n",
    "        start_token = max(start_token + 1, next_start_token)\n",
    "        if start_token >= end_token: start_token = end_token\n",
    "    chunks = [chunk for chunk in chunks if chunk.strip()]\n",
    "    print(f\"Split text into {len(chunks)} chunks (target size: {chunk_size} tokens, overlap: {overlap} tokens).\")\n",
    "    return chunks\n",
    "\n",
    "# --- get_age_from_grade, determine_reading_level, generate_summary, post_process_summary, save_summary functions remain the same ---\n",
    "def get_age_from_grade(grade):\n",
    "    return grade + 5\n",
    "\n",
    "def determine_reading_level(grade):\n",
    "    grade = int(grade); age = get_age_from_grade(grade)\n",
    "    if 1 <= grade <= 3: return \"lower\", f\"early_elementary_gr{grade}_age{age}\"\n",
    "    elif 4 <= grade <= 6: return \"middle\", f\"late_elementary_gr{grade}_age{age}\"\n",
    "    elif 7 <= grade <= 9: return \"higher\", f\"middle_school_gr{grade}_age{age}\"\n",
    "    else: return \"higher\", f\"high_school_gr{grade}_age{age}\"\n",
    "\n",
    "def generate_summary(text_chunks, grade_level, duration, has_math=False):\n",
    "    level_description, _ = determine_reading_level(grade_slider.value)\n",
    "    prompts = { \"lower\": { \"standard\": f\"Summarize the key points from this text for a {get_age_from_grade(grade_slider.value)}-year-old (Grade {grade_slider.value}). Use simple words and short sentences. Focus on the main ideas. Explain any big words simply:\\n\\n\", \"math\": f\"Explain the main math ideas in this text simply for a {get_age_from_grade(grade_slider.value)}-year-old (Grade {grade_slider.value}). Describe any steps clearly. Use easy words. What are the most important math things mentioned?:\\n\\n\" }, \"middle\": { \"standard\": f\"Create an educational summary of this text for a student in Grade {grade_slider.value} (around {get_age_from_grade(grade_slider.value)} years old). Identify the main concepts, important facts, and key takeaways. Use clear language and provide context or simple examples where helpful:\\n\\n\", \"math\": f\"Create an educational summary explaining the mathematical concepts in this text for a Grade {grade_slider.value} student ({get_age_from_grade(grade_slider.value)} years old). Define key terms, explain formulas or processes step-by-step if possible, and state the main mathematical points clearly:\\n\\n\" }, \"higher\": { \"standard\": f\"Generate a concise, comprehensive summary of the following text suitable for a Grade {grade_slider.value} student ({get_age_from_grade(grade_slider.value)} years old). Focus on the core arguments, significant findings, key concepts, and principles. Maintain clarity and accuracy:\\n\\n\", \"math\": f\"Generate a comprehensive summary of the mathematical content in this text for a Grade {grade_slider.value} student ({get_age_from_grade(grade_slider.value)} years old). Clearly explain the main theorems, definitions, formulas, and methodologies discussed. Highlight the significance and application of the concepts presented:\\n\\n\" } }\n",
    "    prompt_type = \"math\" if has_math else \"standard\"\n",
    "    base_prompt = prompts.get(level_description, prompts[\"middle\"]).get(prompt_type, prompts[\"middle\"][\"standard\"])\n",
    "    words_per_minute_map = { \"lower\": 100, \"middle\": 130, \"higher\": 150 }; wpm = words_per_minute_map.get(level_description, 130)\n",
    "    target_words = wpm * duration; min_words = int(target_words * 0.85); max_words = int(target_words * 1.15)\n",
    "    avg_tokens_per_word = 1.4; model_max_gen_length = model.config.max_length\n",
    "    min_tokens_per_chunk = max(30, int((min_words * avg_tokens_per_word * 0.8) / len(text_chunks)))\n",
    "    max_tokens_per_chunk = min(model_max_gen_length // 2, int((max_words * avg_tokens_per_word * 1.2) / len(text_chunks)))\n",
    "    print(f\"Generating {duration}-minute summary ({min_words}-{max_words} target words) for {level_description} level ({'math focused' if has_math else 'standard'}).\"); print(f\"Model max generation length: {model_max_gen_length}. Targeting {min_tokens_per_chunk}-{max_tokens_per_chunk} output tokens per chunk.\")\n",
    "    total_summary = \"\"; total_processed_words = 0\n",
    "    if device == \"cuda\":\n",
    "        try: torch.cuda.empty_cache(); print(f\"Initial GPU Memory Allocated: {torch.cuda.memory_allocated(device) / 1e9:.2f} GB Reserved: {torch.cuda.memory_reserved(device) / 1e9:.2f} GB\")\n",
    "        except Exception as e: print(f\"Could not check GPU memory: {e}\")\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        print(f\"Processing chunk {i+1}/{len(text_chunks)}...\")\n",
    "        input_text = base_prompt + chunk\n",
    "        try: input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=model.config.max_position_embeddings, truncation=True).to(device)\n",
    "        except Exception as e: print(f\"  Error tokenizing chunk {i+1}: {e}\"); continue\n",
    "        if total_processed_words >= max_words * 1.1: print(\"Reached target word count estimate. Stopping generation early.\"); break\n",
    "        try:\n",
    "            current_min_length = min(min_tokens_per_chunk, max_tokens_per_chunk - 10); current_max_length = max(current_min_length + 10, max_tokens_per_chunk)\n",
    "            summary_ids = model.generate( input_ids, num_beams=4, min_length=current_min_length, max_length=current_max_length, length_penalty=1.5, early_stopping=True, no_repeat_ngram_size=3 )\n",
    "            chunk_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "            total_summary += chunk_summary + \"\\n\\n\"; total_processed_words = len(total_summary.split())\n",
    "            print(f\"  Chunk {i+1} summary generated. Total words approx: {total_processed_words}\")\n",
    "            if device == \"cuda\" and i % 5 == 0: torch.cuda.empty_cache()\n",
    "        except torch.cuda.OutOfMemoryError: print(f\"  ‚ùå CUDA Out of Memory Error on chunk {i+1}!\"); print(\"     Try reducing chunk_size or overlap.\"); torch.cuda.empty_cache(); break\n",
    "        except Exception as e: print(f\"  Error generating summary for chunk {i+1}: {type(e).__name__} - {str(e)}\"); continue\n",
    "    print(f\"Finished generating raw summary. Total words approx: {total_processed_words}\")\n",
    "    return post_process_summary(total_summary, level_description, min_words, max_words, has_math)\n",
    "\n",
    "def post_process_summary(summary, grade_level_desc, min_words, max_words, has_math=False):\n",
    "    if not summary or not summary.strip(): print(\"Warning: Raw summary was empty before post-processing.\"); return \"Could not generate a valid summary.\"\n",
    "    potential_points = [s.strip() for s in re.split(r'(?<=[.!?])\\s+|\\n\\n+', summary) if s.strip()]\n",
    "    if not potential_points: print(\"Warning: No points found after splitting the raw summary.\"); return \"Summary content was empty or could not be structured.\"\n",
    "    unique_points = []; seen_points_normalized = set(); min_point_length = 15\n",
    "    for point in potential_points:\n",
    "        normalized_point = ''.join(filter(str.isalnum, point.lower()))\n",
    "        if not normalized_point or len(point) < min_point_length: continue\n",
    "        if normalized_point not in seen_points_normalized: unique_points.append(point); seen_points_normalized.add(normalized_point)\n",
    "    if not unique_points: print(\"Warning: No valid points remained after filtering duplicates/short fragments.\"); return \"Summary content was filtered out during processing.\"\n",
    "    structured_summary_points = []\n",
    "    for point in unique_points:\n",
    "        if re.match(r'^[\\*\\-\\‚Ä¢]\\s+', point): structured_summary_points.append(point)\n",
    "        else: structured_summary_points.append(f\"* {point}\")\n",
    "    title = \"\"; grade_val = grade_slider.value; age = get_age_from_grade(grade_val)\n",
    "    if has_math: title = f\"# Math Summary (Grade {grade_val} / Age {age})\"\n",
    "    else: title = f\"# Summary (Grade {grade_val} / Age {age})\"\n",
    "    processed_summary = title + \"\\n\\n\" + \"\\n\".join(structured_summary_points)\n",
    "    words = processed_summary.split(); current_word_count = len(words)\n",
    "    if current_word_count > max_words:\n",
    "        print(f\"Combined summary ({current_word_count} words) exceeds target ({max_words}). Trimming points...\")\n",
    "        points_to_keep = []; word_count_so_far = len(title.split())\n",
    "        for point in structured_summary_points:\n",
    "             point_word_count = len(point.split())\n",
    "             if word_count_so_far + point_word_count <= max_words: points_to_keep.append(point); word_count_so_far += point_word_count\n",
    "             else:\n",
    "                 remaining_words = max_words - word_count_so_far\n",
    "                 if remaining_words > 10:\n",
    "                     point_words = point.split(); prefix = \"\";\n",
    "                     if point_words[0] in ['*', '-', '‚Ä¢']: prefix = point_words[0] + \" \"; point_words = point_words[1:]\n",
    "                     truncated_point_text = ' '.join(point_words[:remaining_words])\n",
    "                     points_to_keep.append(f\"{prefix}{truncated_point_text}...\"); word_count_so_far += remaining_words + 1\n",
    "                 break\n",
    "        processed_summary = title + \"\\n\\n\" + \"\\n\".join(points_to_keep); final_word_count = len(processed_summary.split())\n",
    "        print(f\"Trimmed summary to {final_word_count} words.\")\n",
    "    elif current_word_count < min_words: print(f\"Warning: Final summary ({current_word_count} words) is shorter than the desired minimum ({min_words} words).\")\n",
    "    return processed_summary\n",
    "\n",
    "def save_summary(summary, filename, grade_level_name, duration):\n",
    "    base_filename = os.path.splitext(filename)[0]; safe_basename = re.sub(r'[^\\w\\-]+', '_', base_filename)\n",
    "    output_filename = f\"{safe_basename}_{grade_level_name}_{duration}min_summary.txt\"\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f: f.write(summary)\n",
    "        print(f\"Summary successfully saved to: {output_filename}\"); return output_filename\n",
    "    except Exception as e: print(f\"Error saving summary to file '{output_filename}': {str(e)}\"); return None\n",
    "\n",
    "# --- UI Widgets (remain the same) ---\n",
    "output = Output()\n",
    "uploader = FileUpload( accept='.pdf', multiple=False, description='Upload PDF', layout=Layout(width='auto'), button_style='primary' )\n",
    "grade_slider = IntSlider(min=1, max=12, step=1, value=8, description='Grade Level:', continuous_update=False)\n",
    "duration_buttons = RadioButtons( options=[('10 min read', 10), ('20 min read', 20), ('30 min read', 30)], value=10, description='Target Time:', disabled=False, layout=Layout(width='auto') )\n",
    "summarize_button = Button( description='Generate Summary', disabled=True, button_style='success', tooltip='Click to generate the summary after uploading a PDF', icon='check', layout=Layout(width='auto', margin='10px 0 0 0') )\n",
    "summary_output = Output()\n",
    "\n",
    "# --- Event Handlers (remain the same, still use global pdf_path) ---\n",
    "def on_upload_change(change):\n",
    "    global pdf_path\n",
    "    summarize_button.disabled = True; pdf_path = None\n",
    "    if change['new']:\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            try:\n",
    "                file_info = change['new'][0]; filename = file_info['name']; content = file_info['content']\n",
    "                print(f\"Processing uploaded file: '{filename}' ({len(content)} bytes)\")\n",
    "                temp_pdf_path = os.path.join(os.getcwd(), filename)\n",
    "                with open(temp_pdf_path, 'wb') as f: f.write(content)\n",
    "                pdf_path = temp_pdf_path\n",
    "                print(f\"‚úÖ File successfully saved locally as: {pdf_path}\")\n",
    "                summarize_button.disabled = False\n",
    "            except Exception as e: print(f\"‚ùå Error saving/processing uploaded file: {e}\"); print(\"\\nUpload Error Traceback:\"); traceback.print_exc()\n",
    "    elif not change['new'] and change['old']:\n",
    "         with output: clear_output(wait=True); print(\"File selection cleared.\")\n",
    "uploader.observe(on_upload_change, names='value')\n",
    "\n",
    "def on_summarize_clicked(b):\n",
    "    global pdf_path, ocr_reader # Make sure ocr_reader is accessible if needed inside\n",
    "    summarize_button.disabled = True; summarize_button.description = 'Processing...'; summarize_button.icon = 'hourglass-half'\n",
    "    if pdf_path is None or not os.path.exists(pdf_path):\n",
    "        with summary_output: clear_output(wait=True); print(\"‚ö†Ô∏è Error: No valid PDF file path found. Please upload a PDF file again.\")\n",
    "        summarize_button.disabled = False; summarize_button.description = 'Generate Summary'; summarize_button.icon = 'check'; return\n",
    "    grade = grade_slider.value; duration = duration_buttons.value\n",
    "    level_description, grade_level_name_for_file = determine_reading_level(grade)\n",
    "    with summary_output:\n",
    "        clear_output(wait=True); print(f\"üöÄ Starting summarization process for: {os.path.basename(pdf_path)}\"); print(f\"Target: Grade {grade} ({level_description}), {duration} min read time.\"); print(\"-\" * 30)\n",
    "        output_file = None # Initialize output_file\n",
    "        summary = \"\"      # Initialize summary\n",
    "        try:\n",
    "            raw_text = extract_text_from_pdf(pdf_path) # Calls the modified version with EasyOCR\n",
    "            if not raw_text or not raw_text.strip(): summary = \"Error: No text extracted from PDF.\"\n",
    "            else:\n",
    "                has_math = detect_math_content(raw_text); print(f\"Mathematical content detected: {'Yes' if has_math else 'No'}\")\n",
    "                print(\"Cleaning extracted text...\"); cleaned_text = clean_text(raw_text)\n",
    "                if not cleaned_text: summary = \"Error: Cleaned text is empty.\"\n",
    "                else:\n",
    "                    print(\"Splitting text into manageable chunks for the model...\")\n",
    "                    text_chunks = split_text_into_chunks(cleaned_text, chunk_size=700, overlap=100)\n",
    "                    if not text_chunks: summary = \"Error: Text splitting resulted in no chunks.\"\n",
    "                    else:\n",
    "                        print(\"Generating summary using the AI model...\")\n",
    "                        summary = generate_summary(text_chunks, level_description, duration, has_math=has_math)\n",
    "                        print(\"\\nSaving the generated summary...\")\n",
    "                        filename = os.path.basename(pdf_path)\n",
    "                        output_file = save_summary(summary, filename, grade_level_name_for_file, duration)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*10 + \" Summary Generation Complete \" + \"=\"*10)\n",
    "            if summary.startswith(\"Error:\"): print(f\"\\n‚ùå {summary}\")\n",
    "            elif output_file:\n",
    "                print(f\"\\n‚úÖ Full summary saved to: {output_file}\"); print(\"\\nüìÑ Summary Preview (first ~200 words):\"); print('-' * 40)\n",
    "                preview_words = summary.split()[:200]; print(' '.join(preview_words) + (\"...\" if len(summary.split()) > 200 else \"\")); print('-' * 40)\n",
    "            else: print(\"\\n‚ö†Ô∏è Summary generated but failed to save to file.\"); print(\"\\nüìÑ Summary Content:\"); print('-' * 40); print(summary); print('-' * 40)\n",
    "        # Removed specific TesseractNotFoundError catch block\n",
    "        except Exception as e: print(f\"\\n‚ùå An unexpected critical error occurred during summarization:\"); print(f\"Error Type: {type(e).__name__}\"); print(f\"Error Details: {str(e)}\"); print(\"\\nTraceback:\"); traceback.print_exc()\n",
    "        finally:\n",
    "            summarize_button.disabled = False; summarize_button.description = 'Generate Summary'; summarize_button.icon = 'check'\n",
    "            if device == \"cuda\":\n",
    "                try: torch.cuda.empty_cache(); print(f\"\\nFinal GPU Memory Allocated: {torch.cuda.memory_allocated(device) / 1e9:.2f} GB Reserved: {torch.cuda.memory_reserved(device) / 1e9:.2f} GB\")\n",
    "                except Exception as e: print(f\"Could not clear/check final GPU memory: {e}\")\n",
    "summarize_button.on_click(on_summarize_clicked)\n",
    "\n",
    "# --- Display UI (remain the same) ---\n",
    "controls = VBox([ HBox([grade_slider, duration_buttons], layout=Layout(justify_content='space-around')), summarize_button ])\n",
    "app_layout = VBox([ Label(value=\"üìö PDF Summarizer for Education üìö\", layout=Layout(display='flex', justify_content='center', font_weight='bold', font_size='20px', margin='10px')), Label(value=\"1. Upload a PDF document:\", layout=Layout(margin='5px 0 0 0')), uploader, output, Label(value=\"2. Select Target Audience and Reading Time:\", layout=Layout(margin='15px 0 0 0')), controls, Label(value=\"3. Summary Output:\", layout=Layout(margin='15px 0 0 0')), summary_output ], layout=Layout(border='1px solid #ccc', padding='15px', width='80%'))\n",
    "display(app_layout)\n",
    "\n",
    "# --- End of Combined Code ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
